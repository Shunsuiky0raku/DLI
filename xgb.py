# -*- coding: utf-8 -*-
"""XGB

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iVZyPRWrqTHmkJGmL9qOjNEKXjCcJPM1
"""

# ==== Section 0: Runtime & GPU setup ====
!nvidia-smi -L || true

# Fresh XGBoost with GPU support; tldextract for optional URL features
!pip -q install --upgrade xgboost==2.0.3 tldextract

import os, sys, platform, numpy as np, pandas as pd
import matplotlib.pyplot as plt

import xgboost as xgb
from xgboost import XGBClassifier

from sklearn.model_selection import train_test_split
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                             f1_score, confusion_matrix, roc_auc_score,
                             classification_report, roc_curve, precision_recall_curve, auc)
from sklearn.impute import SimpleImputer

print("Python:", sys.version)
print("XGBoost:", xgb.__version__)
print("GPU build detected:", xgb.rabit.get_rank is not None)  # just a quick sanity print

# Colab GPU hint (falls back automatically if GPU unavailable)
USE_GPU = True
TREE_METHOD = "gpu_hist" if USE_GPU else "hist"
PREDICTOR = "gpu_predictor" if USE_GPU else "auto"

# ==== Section 1: Load data from Drive ====
from google.colab import drive
drive.mount('/content/drive')

CSV_PATH = "/content/drive/MyDrive/Individual Assignment/PhiUSIIL_Phishing_URL_Dataset.csv"

# Read CSV safely (handles big files)
df = pd.read_csv(CSV_PATH, low_memory=False)
print("Shape:", df.shape)
df.head(3)

# ==== Section 2: Detect/normalize the label column ====
possible_labels = [
    'label','Label','LABEL','target','Target','class','Class','result','Result',
    'status','Status','phishing','is_phishing','malicious','y','Y'
]

label_col = None
for c in df.columns:
    if c in possible_labels:
        label_col = c
        break

# If still not found, try common binary columns by inspection
if label_col is None:
    # Heuristic: choose a column with exactly 2 unique values and not 'url'
    candidates = [c for c in df.columns if c.lower() not in ['url','urls'] and df[c].nunique(dropna=True) in [2]]
    label_col = candidates[0] if candidates else None

if label_col is None:
    raise ValueError("Could not auto-detect label column. Please rename your label column to 'label' and re-run.")

print("Detected label column:", label_col)

def normalize_labels(s: pd.Series) -> pd.Series:
    x = s.astype(str).str.strip().str.lower()
    mapping = {
        '1':1,'true':1,'yes':1,'phish':1,'phishing':1,'malicious':1,'bad':1,'attack':1,'spam':1,
        '0':0,'false':0,'no':0,'legit':0,'legitimate':0,'benign':0,'good':0
    }
    y = x.map(mapping)
    if y.isna().any():
        # numeric fallback
        y_num = pd.to_numeric(x, errors='coerce')
        if set(y_num.dropna().unique()).issubset({0,1}):
            y = y.fillna(y_num)
    # final fallback: keyword contains => 1 else 0
    y = y.fillna(x.str.contains(r'phish|malic|attack|spam', regex=True)).astype(int)
    return y

y_all = normalize_labels(df[label_col])
print("Class balance:\n", y_all.value_counts())

# ==== PATCH to Section 3: Always build lexical features if URL present ====
from urllib.parse import urlparse
import tldextract, re

def is_ip(host):
    return bool(re.fullmatch(r'(?:\d{1,3}\.){3}\d{1,3}', str(host or '')))

def url_lexical_features(u: str) -> dict:
    u = str(u or "")
    parsed = urlparse(u)
    host = parsed.netloc or ""
    path = parsed.path or ""
    ext = tldextract.extract(u)
    subd_count = len([s for s in ext.subdomain.split('.') if s]) if ext.subdomain else 0
    return {
        "len_url": len(u),
        "len_host": len(host),
        "len_path": len(path),
        "num_dots": u.count('.'),
        "num_hyphens": u.count('-'),
        "num_ats": u.count('@'),
        "num_qmarks": u.count('?'),
        "num_equals": u.count('='),
        "num_slashes": u.count('/'),
        "num_percents": u.count('%'),
        "num_digits": sum(ch.isdigit() for ch in u),
        "ratio_digits": (sum(ch.isdigit() for ch in u) / max(1, len(u))),
        "has_https": int(u.lower().startswith("https")),
        "is_ip_host": int(is_ip(host)),
        "subdomain_count": subd_count,
        "tld_len": len(ext.suffix or ""),
    }

# Detect URL column (if any)
url_col = next((c for c in df.columns if c.strip().lower() in {"url","urls"}), None)

# Numeric features from dataset (drop label + obvious text)
drop_cols = {label_col}
if url_col: drop_cols.add(url_col)
non_numeric = set([c for c in df.columns if df[c].dtype == 'object'])
drop_cols |= non_numeric
X_num = df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore').select_dtypes(include=[np.number])

# Lexical features (if URL exists)
if url_col:
    X_lex = pd.DataFrame([url_lexical_features(u) for u in df[url_col].astype(str)])
else:
    X_lex = pd.DataFrame()  # no URL column

# Main model uses numeric features (as before)
X = X_num.copy()
print("X_num shape:", X_num.shape, "| X_lex shape:", X_lex.shape)
