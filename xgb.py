
# -*- coding: utf-8 -*-
"""XGB

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iVZyPRWrqTHmkJGmL9qOjNEKXjCcJPM1
"""

# ==== Section 0: Runtime & GPU setup ====
!nvidia-smi -L || true

# Fresh XGBoost with GPU support; tldextract for optional URL features
!pip -q install --upgrade xgboost==2.0.3 tldextract

import os, sys, platform, numpy as np, pandas as pd
import matplotlib.pyplot as plt

import xgboost as xgb
from xgboost import XGBClassifier

from sklearn.model_selection import train_test_split
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                             f1_score, confusion_matrix, roc_auc_score,
                             classification_report, roc_curve, precision_recall_curve, auc)
from sklearn.impute import SimpleImputer

print("Python:", sys.version)
print("XGBoost:", xgb.__version__)
print("GPU build detected:", xgb.rabit.get_rank is not None)  # just a quick sanity print

# Colab GPU hint (falls back automatically if GPU unavailable)
USE_GPU = True
TREE_METHOD = "gpu_hist" if USE_GPU else "hist"
PREDICTOR = "gpu_predictor" if USE_GPU else "auto"

# ==== Section 1: Load data from Drive ====
from google.colab import drive
drive.mount('/content/drive')

CSV_PATH = "/content/drive/MyDrive/Individual Assignment/PhiUSIIL_Phishing_URL_Dataset.csv"

# Read CSV safely (handles big files)
df = pd.read_csv(CSV_PATH, low_memory=False)
print("Shape:", df.shape)
df.head(3)

# ==== Section 2: Detect/normalize the label column ====
possible_labels = [
    'label','Label','LABEL','target','Target','class','Class','result','Result',
    'status','Status','phishing','is_phishing','malicious','y','Y'
]

label_col = None
for c in df.columns:
    if c in possible_labels:
        label_col = c
        break

# If still not found, try common binary columns by inspection
if label_col is None:
    # Heuristic: choose a column with exactly 2 unique values and not 'url'
    candidates = [c for c in df.columns if c.lower() not in ['url','urls'] and df[c].nunique(dropna=True) in [2]]
    label_col = candidates[0] if candidates else None

if label_col is None:
    raise ValueError("Could not auto-detect label column. Please rename your label column to 'label' and re-run.")

print("Detected label column:", label_col)

def normalize_labels(s: pd.Series) -> pd.Series:
    x = s.astype(str).str.strip().str.lower()
    mapping = {
        '1':1,'true':1,'yes':1,'phish':1,'phishing':1,'malicious':1,'bad':1,'attack':1,'spam':1,
        '0':0,'false':0,'no':0,'legit':0,'legitimate':0,'benign':0,'good':0
    }
    y = x.map(mapping)
    if y.isna().any():
        # numeric fallback
        y_num = pd.to_numeric(x, errors='coerce')
        if set(y_num.dropna().unique()).issubset({0,1}):
            y = y.fillna(y_num)
    # final fallback: keyword contains => 1 else 0
    y = y.fillna(x.str.contains(r'phish|malic|attack|spam', regex=True)).astype(int)
    return y

y_all = normalize_labels(df[label_col])
print("Class balance:\n", y_all.value_counts())

# ==== PATCH to Section 3: Always build lexical features if URL present ====
from urllib.parse import urlparse
import tldextract, re

def is_ip(host):
    return bool(re.fullmatch(r'(?:\d{1,3}\.){3}\d{1,3}', str(host or '')))

def url_lexical_features(u: str) -> dict:
    u = str(u or "")
    parsed = urlparse(u)
    host = parsed.netloc or ""
    path = parsed.path or ""
    ext = tldextract.extract(u)
    subd_count = len([s for s in ext.subdomain.split('.') if s]) if ext.subdomain else 0
    return {
        "len_url": len(u),
        "len_host": len(host),
        "len_path": len(path),
        "num_dots": u.count('.'),
        "num_hyphens": u.count('-'),
        "num_ats": u.count('@'),
        "num_qmarks": u.count('?'),
        "num_equals": u.count('='),
        "num_slashes": u.count('/'),
        "num_percents": u.count('%'),
        "num_digits": sum(ch.isdigit() for ch in u),
        "ratio_digits": (sum(ch.isdigit() for ch in u) / max(1, len(u))),
        "has_https": int(u.lower().startswith("https")),
        "is_ip_host": int(is_ip(host)),
        "subdomain_count": subd_count,
        "tld_len": len(ext.suffix or ""),
    }

# Detect URL column (if any)
url_col = next((c for c in df.columns if c.strip().lower() in {"url","urls"}), None)

# Numeric features from dataset (drop label + obvious text)
drop_cols = {label_col}
if url_col: drop_cols.add(url_col)
non_numeric = set([c for c in df.columns if df[c].dtype == 'object'])
drop_cols |= non_numeric
X_num = df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore').select_dtypes(include=[np.number])

# Lexical features (if URL exists)
if url_col:
    X_lex = pd.DataFrame([url_lexical_features(u) for u in df[url_col].astype(str)])
else:
    X_lex = pd.DataFrame()  # no URL column

# Main model uses numeric features (as before)
X = X_num.copy()
print("X_num shape:", X_num.shape, "| X_lex shape:", X_lex.shape)

# ==== Section 4: Split & impute ====
X_train, X_test, y_train, y_test = train_test_split(
    X, y_all, test_size=0.20, random_state=42, stratify=y_all
)

# Further carve out a small validation set from training for early stopping
X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train, test_size=0.15, random_state=42, stratify=y_train
)

imp = SimpleImputer(strategy="median")
X_train = imp.fit_transform(X_train)
X_val   = imp.transform(X_val)
X_test  = imp.transform(X_test)

pos = (y_train == 1).sum()
neg = (y_train == 0).sum()
scale_pos_weight = max(1.0, neg / max(1, pos))
print(f"scale_pos_weight ~ {scale_pos_weight:.2f} (neg={neg}, pos={pos})")

# ==== Section 5A: Train XGBoost (GPU) with early stopping ====
params = dict(
    tree_method=TREE_METHOD,
    predictor=PREDICTOR,
    n_estimators=2000,            # with early stopping; will stop much earlier
    max_depth=8,
    learning_rate=0.07,
    subsample=0.9,
    colsample_bytree=0.8,
    min_child_weight=1,
    reg_alpha=0.0,
    reg_lambda=1.0,
    objective='binary:logistic',
    random_state=42,
    n_jobs=-1,
    eval_metric='logloss',
    scale_pos_weight=scale_pos_weight
)

model = XGBClassifier(**params)
model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    verbose=False,
    early_stopping_rounds=75
)

print("Best iteration:", model.get_booster().best_iteration)

# ==== Section 5B: Train a small lexical-only XGBoost (for URL-only testing) ====
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from xgboost import XGBClassifier
import joblib, json, os

if url_col and not X_lex.empty:
    Xl_train, Xl_test, yl_train, yl_test = train_test_split(
        X_lex, y_all, test_size=0.20, random_state=42, stratify=y_all
    )
    Xl_train, Xl_val, yl_train, yl_val = train_test_split(
        Xl_train, yl_train, test_size=0.15, random_state=42, stratify=yl_train
    )
    imp_lex = SimpleImputer(strategy="median")
    Xl_train = imp_lex.fit_transform(Xl_train)
    Xl_val   = imp_lex.transform(Xl_val)
    Xl_test  = imp_lex.transform(Xl_test)

    pos = (yl_train == 1).sum(); neg = (yl_train == 0).sum()
    spw = max(1.0, neg / max(1, pos))

    xgb_lex = XGBClassifier(
        tree_method=TREE_METHOD, predictor=PREDICTOR,
        n_estimators=800, max_depth=7, learning_rate=0.08,
        subsample=0.9, colsample_bytree=0.8, min_child_weight=1,
        reg_lambda=1.0, objective='binary:logistic', random_state=42,
        n_jobs=-1, eval_metric='logloss', scale_pos_weight=spw
    )
    xgb_lex.fit(Xl_train, yl_train,
                eval_set=[(Xl_val, yl_val)],
                verbose=False, early_stopping_rounds=50)

    # save artifacts
    os.makedirs("/content/artifacts", exist_ok=True)
    joblib.dump(xgb_lex, "/content/artifacts/xgb_phishing_lexical.pkl")
    joblib.dump(imp_lex, "/content/artifacts/imputer_lexical.pkl")
    with open("/content/artifacts/feature_names_lex.json","w") as f:
        json.dump(list(X_lex.columns), f)

    print("Lexical-only model saved.")
else:
    print("No URL column found or lexical features empty; skipping lexical-only model.")

# ==== Section 6: Tune decision threshold for best F1 ====
val_proba = model.predict_proba(X_val)[:,1]
prec, rec, thr = precision_recall_curve(y_val, val_proba)
f1s = 2*prec*rec/(prec+rec+1e-12)
best_ix = int(np.nanargmax(f1s))
best_thr = thr[max(0, best_ix-1)] if best_ix < len(thr) else 0.5  # guard
print(f"Best F1 on val: {f1s[best_ix]:.4f} at threshold {best_thr:.4f}")
# ==== Section 7: Evaluate on held-out test set ====
test_proba = model.predict_proba(X_test)[:,1]
y_pred = (test_proba >= best_thr).astype(int)

acc = accuracy_score(y_test, y_pred)
pre = precision_score(y_test, y_pred, zero_division=0)
rec = recall_score(y_test, y_pred, zero_division=0)
f1  = f1_score(y_test, y_pred, zero_division=0)
roc = roc_auc_score(y_test, test_proba)

# PR-AUC
p, r, _ = precision_recall_curve(y_test, test_proba)
pr_auc = auc(r, p)

print(f"Test Accuracy : {acc:.4f}")
print(f"Test Precision: {pre:.4f}")
print(f"Test Recall   : {rec:.4f}")
print(f"Test F1       : {f1:.4f}")
print(f"ROC-AUC       : {roc:.4f}")
print(f"PR-AUC        : {pr_auc:.4f}\n")

print("Classification report:\n", classification_report(y_test, y_pred, digits=4))

cm = confusion_matrix(y_test, y_pred)
tn, fp, fn, tp = cm.ravel()
print("Confusion Matrix [tn fp; fn tp]:")
print(cm)

# Curves (kept minimal & fast)
plt.figure()
fpr, tpr, _ = roc_curve(y_test, test_proba)
plt.plot(fpr, tpr, label=f'ROC AUC={roc:.3f}')
plt.plot([0,1],[0,1],'--')
plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC Curve'); plt.legend(); plt.show()

plt.figure()
plt.plot(r, p, label=f'PR AUC={pr_auc:.3f}')
plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('Precision-Recall'); plt.legend(); plt.show()

# ==== Section 8: Persist model & helper ====
import os, json, joblib, pandas as pd

os.makedirs("/content/artifacts", exist_ok=True)

# 1) Save the trained MAIN model + imputer
joblib.dump(model, "/content/artifacts/xgb_phishing.pkl")
joblib.dump(imp,   "/content/artifacts/imputer.pkl")

# 2) Save the exact feature schema used during training (column order matters)
if isinstance(X, pd.DataFrame):
    feature_names = list(X.columns)
else:
    # If X isn't a DataFrame here, raise a clear error so we don't save junk
    raise RuntimeError(
        "Section 8: 'X' is not a DataFrame. "
        "Please run this cell before converting X to arrays, or keep a copy of the original X."
    )

with open("/content/artifacts/feature_names.json","w") as f:
    json.dump(feature_names, f)

# 3) (Recommended) Persist the tuned threshold for F1 so testing cells can reuse it
if 'best_thr' in globals():
    with open("/content/artifacts/threshold.json","w") as f:
        json.dump({"best_thr": float(best_thr)}, f)

print("Saved to /content/artifacts")
print(os.listdir("/content/artifacts"))

def predict_batch(df_like: pd.DataFrame, threshold: float = None):
    """
    Pass a DataFrame with the same numeric feature columns used in training.
    Returns predicted label (0/1) and probability.
    """
    if threshold is None:
        # Reuse tuned threshold if available; else default to 0.5
        threshold = float(best_thr) if 'best_thr' in globals() else 0.5
    Xt = imp.transform(df_like)
    proba = model.predict_proba(Xt)[:, 1]
    pred = (proba >= threshold).astype(int)
    return pred, proba

# # Example (commented)
# sample = pd.DataFrame(X.head(5))   # uses the same feature columns
# yhat, p = predict_batch(sample)
# print(yhat, p[:5])

# ==== Section 9 (revised): URL scoring utilities with full/lexical modes ====
import re, json, os
import numpy as np
import pandas as pd
from urllib.parse import urlparse
import tldextract, joblib

# Load best_thr (same as before)
thr_path = "/content/artifacts/threshold.json"
if os.path.exists(thr_path):
    try:
        with open(thr_path, "r") as f:
            best_thr = float(json.load(f).get("best_thr", 0.5))
    except Exception:
        best_thr = 0.5
else:
    if 'best_thr' not in globals():
        best_thr = 0.5

# Load main artifacts if present
model_path_main = "/content/artifacts/xgb_phishing.pkl"
imp_path_main   = "/content/artifacts/imputer.pkl"
feat_path_main  = "/content/artifacts/feature_names.json"

model_main = joblib.load(model_path_main) if os.path.exists(model_path_main) else None
imp_main   = joblib.load(imp_path_main)   if os.path.exists(imp_path_main)   else None
feature_names_main = None
if os.path.exists(feat_path_main):
    with open(feat_path_main,"r") as f: feature_names_main = json.load(f)

# Load lexical artifacts (for URL-only scoring)
model_path_lex = "/content/artifacts/xgb_phishing_lexical.pkl"
imp_path_lex   = "/content/artifacts/imputer_lexical.pkl"
feat_path_lex  = "/content/artifacts/feature_names_lex.json"

model_lex = joblib.load(model_path_lex) if os.path.exists(model_path_lex) else None
imp_lex   = joblib.load(imp_path_lex)   if os.path.exists(imp_path_lex)   else None
feature_names_lex = None
if os.path.exists(feat_path_lex):
    with open(feat_path_lex,"r") as f: feature_names_lex = json.load(f)

def is_ip(host):
    return bool(re.fullmatch(r'(?:\d{1,3}\.){3}\d{1,3}', str(host or '')))

def url_lexical_features(u: str) -> dict:
    u = str(u or "")
    parsed = urlparse(u)
    host = parsed.netloc or ""
    path = parsed.path or ""
    ext = tldextract.extract(u)
    subd_count = len([s for s in ext.subdomain.split('.') if s]) if ext.subdomain else 0
    return {
        "len_url": len(u),
        "len_host": len(host),
        "len_path": len(path),
        "num_dots": u.count('.'),
        "num_hyphens": u.count('-'),
        "num_ats": u.count('@'),
        "num_qmarks": u.count('?'),
        "num_equals": u.count('='),
        "num_slashes": u.count('/'),
        "num_percents": u.count('%'),
        "num_digits": sum(ch.isdigit() for ch in u),
        "ratio_digits": (sum(ch.isdigit() for ch in u) / max(1, len(u))),
        "has_https": int(u.lower().startswith("https")),
        "is_ip_host": int(is_ip(host)),
        "subdomain_count": subd_count,
        "tld_len": len(ext.suffix or ""),
    }

def make_feature_df_from_urls(urls, feature_names):
    lex = pd.DataFrame([url_lexical_features(u) for u in urls])
    # IMPORTANT: initialize with NaN so SimpleImputer fills medians
    X_df_full = pd.DataFrame(np.nan, index=lex.index, columns=feature_names)
    for c in lex.columns:
        if c in X_df_full.columns:
            X_df_full[c] = lex[c]
    return X_df_full[feature_names]

def score_urls(urls, threshold: float = None, mode: str = "lexical", return_df: bool = False):
    """
    mode="lexical": use the lexical-only model (URL-only testing)
    mode="full":    attempt to use the main model (requires full feature row)
    """
    if threshold is None:
        threshold = float(best_thr)

    if mode == "lexical":
        if not (model_lex and imp_lex and feature_names_lex):
            raise RuntimeError("Lexical model artifacts not found. Re-run Section 5B.")
        X_df = make_feature_df_from_urls(urls, feature_names_lex)
        Xt = imp_lex.transform(X_df)
        proba = model_lex.predict_proba(Xt)[:,1]
    else:
        if not (model_main and imp_main and feature_names_main):
            raise RuntimeError("Main model artifacts not found.")
        # WARNING: Without the full numeric feature vector, this will be weak.
        X_df = make_feature_df_from_urls(urls, feature_names_main)
        Xt = imp_main.transform(X_df)
        proba = model_main.predict_proba(Xt)[:,1]

    pred = (proba >= threshold).astype(int)
    out = pd.DataFrame({
        "url": urls,
        "phish_probability": proba,
        "prediction": np.where(pred==1, "PHISHING", "LEGIT")
    }).sort_values("phish_probability", ascending=False).reset_index(drop=True)
    if return_df:
        return out
    else:
        from IPython.display import display
        display(out)

# ==== Section 10A (unchanged UI, but now defaults to lexical mode) ====
user_url = input("Enter a URL to test: ").strip()
if not user_url:
    print("No URL entered.")
else:
    res = score_urls([user_url], mode="lexical", return_df=True)
    row = res.iloc[0]
    print(f"\nURL: {row['url']}")
    print(f"Prediction: {row['prediction']}  "
          f"(prob={row['phish_probability']:.4f}, threshold={float(best_thr):.3f})")

# ==== Section 10B: Interactive Threshold widget tester ====
# Enter a URL, adjust threshold, and click "Score URL"
!pip -q install ipywidgets
import ipywidgets as widgets
from IPython.display import display

url_box = widgets.Text(
    value="https://example.com",
    placeholder="Paste a URL…",
    description="URL:",
    layout=widgets.Layout(width="80%")
)
thr_slider = widgets.FloatSlider(value=float(best_thr), min=0.0, max=1.0, step=0.01, description="Threshold")
out = widgets.Output()

def on_click(_):
    out.clear_output()
    urls = [url_box.value.strip()] if url_box.value.strip() else []
    if not urls:
        with out: print("Please enter a URL.")
        return
    df = score_urls(urls, threshold=float(thr_slider.value), return_df=True)
    with out: display(df)

button = widgets.Button(description="Score URL", button_style="primary")
button.on_click(on_click)

display(url_box, thr_slider, button, out)
